# Mini-BERT C++ Implementation

ImplementaciÃ³n de un encoder BERT mini en C++ usando Eigen, basado en el modelo `sentence-transformers/all-MiniLM-L6-v2`.

## ğŸš€ Setup RÃ¡pido

### OpciÃ³n 1: Script automÃ¡tico
```bash
# Crear entorno virtual
./setup_simple.sh

# Luego ejecutar (todo en una lÃ­nea):
source venv/bin/activate && pip install -r requirements.txt && python3 download_weigths.py
```

### OpciÃ³n 2: Manual paso a paso
```bash
# Crear entorno virtual
python3 -m venv venv

# Activar entorno virtual
source venv/bin/activate

# Instalar dependencias
pip install -r requirements.txt
```

### 2. Descargar pesos del modelo
```bash
# AsegÃºrate de tener el entorno virtual activado
python3 download_weigths.py
```

### 3. Instalar Eigen (C++)
```bash
# Ubuntu/Debian
sudo apt install libeigen3-dev

# macOS
brew install eigen

# O descargar manualmente desde https://eigen.tuxfamily.org/
```

### 4. Compilar proyecto
```bash
# Primera vez - configurar y compilar
mkdir build && cd build
cmake ..
make

# Recompilaciones posteriores (solo si cambias cÃ³digo C++)
cd build && make
```

### 5. Ejecutar
```bash
# Tokenizar una frase (con entorno virtual activado)
python3 tokenizer.py

# Ejecutar el modelo C++ (desde directorio build)
./PROYECTO_TOPICOS_IA

# O desde directorio raÃ­z
./build/PROYECTO_TOPICOS_IA
```

## ğŸ“ Estructura del Proyecto

```
â”œâ”€â”€ include/           # Headers C++
â”‚   â”œâ”€â”€ Config.h      # Sistema de configuraciÃ³n
â”‚   â”œâ”€â”€ Embeddings.h  # Capa de embeddings
â”‚   â””â”€â”€ ...
â”œâ”€â”€ src/              # Implementaciones C++
â”œâ”€â”€ weights/          # Pesos del modelo (generado)
â”œâ”€â”€ words/            # Vocabulario
â”‚   â””â”€â”€ vocab.txt
â”œâ”€â”€ download_weigths.py # Script de descarga automÃ¡tica
â”œâ”€â”€ tokenizer.py      # Tokenizador Python
â””â”€â”€ requirements.txt  # Dependencias Python
```

## ğŸ§  Arquitectura

- **Embeddings**: Word + Position + LayerNorm
- **Transformer**: 6 capas con atenciÃ³n multi-head
- **Dimensiones**: 384-dimensional embeddings
- **Vocabulario**: 30,522 tokens

## ğŸ“ Uso

```cpp
// Cargar configuraciÃ³n
Config& config = Config::getInstance();

// Crear capa de embeddings
EmbeddingLayer emb(
    config.getWordEmbeddingsPath(),
    config.getPositionEmbeddingsPath(),
    config.getLayerNormWeightPath(),
    config.getLayerNormBiasPath()
);

// Tokenizar y procesar
std::vector<int> tokens = {101, 6207, 2003, 2919, 102}; // [CLS] hello world [SEP]
Eigen::MatrixXf embeddings = emb.forward(tokens);
```

## ğŸ”§ ConfiguraciÃ³n

El proyecto usa un sistema de configuraciÃ³n flexible que detecta automÃ¡ticamente los paths:

- **Pesos**: `weights/pesos_comprimidos/content/pesos_csv/`
- **Tokens**: `tokens.csv` 
- **Vocabulario**: `words/vocab.txt`

## âš¡ Comandos de Desarrollo

```bash
# Solo recompilar despuÃ©s de cambios en C++
cd build && make

# Solo ejecutar (si ya estÃ¡ compilado)
./build/PROYECTO_TOPICOS_IA

# Regenerar tokens y ejecutar
source venv/bin/activate && python3 tokenizer.py && ./build/PROYECTO_TOPICOS_IA
```

## ğŸ“Š Estado del Desarrollo

- âœ… Sistema de configuraciÃ³n
- âœ… Carga de pesos CSV
- âœ… Embeddings (Word + Position + LayerNorm)
- â³ Capas Transformer
- â³ AtenciÃ³n Multi-Head
- â³ Feed-Forward
- â³ Pipeline completo

## ğŸ¯ Objetivo

Crear un encoder BERT funcional capaz de:
- Procesar frases en lenguaje natural
- Generar embeddings semÃ¡nticos
- Calcular similitud entre frases

## ğŸ“‹ Requisitos

- **C++20** o superior
- **Eigen 3.3+** para operaciones matriciales
- **Python 3.7+** para descarga de pesos y tokenizaciÃ³n
- **CMake 3.16+** para compilaciÃ³n